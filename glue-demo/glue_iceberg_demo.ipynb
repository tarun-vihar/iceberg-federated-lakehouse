{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup libs\n",
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from py4j.java_gateway import java_import\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from config.py\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/tarunvihartumati/iceberg-projects/glue-demo')\n",
    "from config import *\n",
    "\n",
    "# Set AWS credentials\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "\n",
    "# Print configuration\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Profile is set in the cell above via os.environ\n",
    "# Shell export commands don't persist in Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables are now loaded from config.py\n",
    "# bucket_name, bucket_prefix, database_name, warehouse_path are all defined in config\n",
    "\n",
    "print(f\"Bucket: {S3_BUCKET_NAME}\")\n",
    "print(f\"Prefix: {S3_BUCKET_PREFIX}\")\n",
    "print(f\"Database: {DATABASE_NAME}\")\n",
    "print(f\"Warehouse: {S3_WAREHOUSE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Fix for connection refused error - set local IP before creating Spark session\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "\n",
    "# IMPORTANT: Stop any existing Spark session first\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"ðŸ›‘ Stopped existing Spark session\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Use configuration from config.py\n",
    "jars_dir = os.path.expanduser(JARS_DIR)\n",
    "jars = [f\"{jars_dir}/{jar}\" for jar in JAR_FILES]\n",
    "\n",
    "# Build driver classpath string\n",
    "driver_classpath = \":\".join(jars)\n",
    "\n",
    "# Get Spark configuration from config file\n",
    "spark_config = get_spark_config()\n",
    "\n",
    "# Create Spark session with configuration\n",
    "builder = SparkSession.builder\n",
    "for key, value in spark_config.items():\n",
    "    builder = builder.config(key, value)\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark session created successfully with Glue catalog\")\n",
    "print(f\"âœ… Using warehouse: {S3_WAREHOUSE_PATH}\")\n",
    "print(f\"âœ… Using region: {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import time\n",
    "\n",
    "ut = time.time()\n",
    "\n",
    "product = [\n",
    "    {'id': '00001', 'name': 'Heater','price': 99900.76 },\n",
    "    {'id': '00002', 'name': 'Thermostat','price': 881500.00},\n",
    "    {'id': '00003', 'name': 'Television','price': 1400.89},\n",
    "    {'id': '00004', 'name': 'Blender','price': 300.99},\n",
    "    {'id': '00005', 'name': 'USB charger','price': 400.00}\n",
    "]\n",
    "\n",
    "df_products = spark.createDataFrame(Row(**x) for x in product)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_products.createOrReplaceTempView(\"tmp_product\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the temporary view\n",
    "spark.sql(\"SELECT *  FROM tmp_product order by 1 LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database using config\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {CATALOG_NAME}.{DATABASE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists using config\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {FULL_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table using config\n",
    "sql_stmnt = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {FULL_TABLE_NAME} (\n",
    "    id string,\n",
    "    name string,\n",
    "    price decimal(10,2)\n",
    ")\n",
    "USING iceberg\n",
    "TBLPROPERTIES (\n",
    "'table_type'='{ICEBERG_TABLE_TYPE}', \n",
    "'format-version'='{ICEBERG_FORMAT_VERSION}'\n",
    ")\n",
    "LOCATION '{TABLE_LOCATION}'\n",
    "\"\"\"\n",
    "print(f\"Executing SparkSQL:\\n`{sql_stmnt}`\")\n",
    "spark.sql(sql_stmnt).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List S3 contents using config\n",
    "!aws s3 ls '{TABLE_LOCATION}' --recursive --profile {AWS_PROFILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESC EXTENDED {FULL_TABLE_NAME}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {FULL_TABLE_NAME} \n",
    "SELECT * FROM tmp_product\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME}.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME}.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT h.made_current_at, s.operation, \n",
    "h.snapshot_id, h.is_current_ancestor, \n",
    "s.summary[\"spark.app.id\"] \n",
    "FROM {FULL_TABLE_NAME}.history h \n",
    "JOIN {FULL_TABLE_NAME}.snapshots s  \n",
    "ON h.snapshot_id = s.snapshot_id \n",
    "ORDER BY made_current_at\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DELETE FROM {FULL_TABLE_NAME} WHERE name = \"Blender\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut = time.time()\n",
    "\n",
    "product_updates = [\n",
    "    {'id': '00001', 'name': 'Heater', 'price': 400}, # Update\n",
    "    {'id': '00006', 'name': 'Chair', 'price': 500} # Insert\n",
    "]\n",
    "df_product_updates = spark.createDataFrame(Row(**x) for x in product_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_product_updates.createOrReplaceTempView(f\"tmp_prodct_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM tmp_prodct_updates\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "MERGE INTO glue.prod.customer AS t\n",
    "USING (SELECT * FROM tmp_prodct_updates) AS u\n",
    "ON t.id = u.id\n",
    "WHEN MATCHED THEN UPDATE SET t.price = u.price\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME}.snapshots ORDER BY committed_at DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current snapshots available in the table\n",
    "print(\"=== Available Snapshots ===\")\n",
    "snapshots_df = spark.sql(f\"SELECT committed_at, snapshot_id, operation FROM {FULL_TABLE_NAME}.snapshots ORDER BY committed_at\")\n",
    "snapshots_df.show(truncate=False)\n",
    "\n",
    "# Get snapshot IDs dynamically\n",
    "snapshots = snapshots_df.collect()\n",
    "\n",
    "if len(snapshots) > 0:\n",
    "    first_snapshot_id = snapshots[0]['snapshot_id']\n",
    "    print(f\"\\n=== Querying first snapshot (ID: {first_snapshot_id}) ===\")\n",
    "    spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME} VERSION AS OF {first_snapshot_id}\").show()\n",
    "else:\n",
    "    print(\"No snapshots found in the table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query different snapshots if they exist\n",
    "snapshots_df = spark.sql(f\"SELECT committed_at, snapshot_id, operation FROM {FULL_TABLE_NAME}.snapshots ORDER BY committed_at\")\n",
    "snapshots = snapshots_df.collect()\n",
    "\n",
    "if len(snapshots) >= 2:\n",
    "    second_snapshot_id = snapshots[1]['snapshot_id']\n",
    "    print(f\"=== Querying second snapshot (ID: {second_snapshot_id}) ===\")\n",
    "    spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME} VERSION AS OF {second_snapshot_id}\").show()\n",
    "    \n",
    "if len(snapshots) >= 3:\n",
    "    third_snapshot_id = snapshots[2]['snapshot_id']\n",
    "    print(f\"\\n=== Querying third snapshot (ID: {third_snapshot_id}) ===\")\n",
    "    spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME} VERSION AS OF {third_snapshot_id}\").show()\n",
    "\n",
    "# Show comparison\n",
    "print(f\"\\n=== Current table state ===\")\n",
    "spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
